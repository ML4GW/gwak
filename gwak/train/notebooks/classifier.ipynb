{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "e7989a1e-3111-4570-9261-cf681c0d0b8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, roc_curve\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "from scipy.special import logit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "e92ca577-70f3-4c1e-81e6-67f5089016a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train: (199936, 16)\n",
      "y_train: (199936,)\n",
      "context_train: (199936, 1)\n",
      "X_test (signals): (199936, 16)\n",
      "y_test (signals): (199936,)\n",
      "context_test (signals): (199936, 1)\n",
      "X_bkg: (99968, 16)\n",
      "y_bkg: (99968,)\n",
      "context_bkg: (99968, 1)\n"
     ]
    }
   ],
   "source": [
    "# Load classifier training data\n",
    "X_train = np.load('/home/katya.govorkova/gwak2/gwak/output/ResNet_wnb_HL/embeddings.npy')\n",
    "y_train_full = np.load('/home/katya.govorkova/gwak2/gwak/output/ResNet_wnb_HL/labels.npy')\n",
    "context_train = np.load('/home/katya.govorkova/gwak2/gwak/output/ResNet_wnb_HL/correlations.npy')\n",
    "\n",
    "# Load classifier + NF test data (signals)\n",
    "X_test = np.load('/home/katya.govorkova/gwak2/gwak/output/ResNet_signals_HL/embeddings.npy')\n",
    "y_test_full = np.load('/home/katya.govorkova/gwak2/gwak/output/ResNet_signals_HL/labels.npy')\n",
    "context_test = np.load('/home/katya.govorkova/gwak2/gwak/output/ResNet_signals_HL/correlations.npy')\n",
    "\n",
    "# Load second test set (background)\n",
    "X_bkg = np.load('/home/katya.govorkova/gwak2/gwak/output/ResNet_HL/embeddings.npy')\n",
    "y_bkg_full = np.load('/home/katya.govorkova/gwak2/gwak/output/ResNet_HL/labels.npy')\n",
    "context_bkg = np.load('/home/katya.govorkova/gwak2/gwak/output/ResNet_HL/correlations.npy')\n",
    "\n",
    "# Print shapes for sanity check\n",
    "print(\"X_train:\", X_train.shape)\n",
    "print(\"y_train:\", y_train_full.shape)\n",
    "print(\"context_train:\", context_train.shape)\n",
    "\n",
    "print(\"X_test (signals):\", X_test.shape)\n",
    "print(\"y_test (signals):\", y_test_full.shape)\n",
    "print(\"context_test (signals):\", context_test.shape)\n",
    "\n",
    "print(\"X_bkg:\", X_bkg.shape)\n",
    "print(\"y_bkg:\", y_bkg_full.shape)\n",
    "print(\"context_bkg:\", context_bkg.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "d77893b5-6737-4a7b-9558-c38f8d8d54c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] Signals: 99968, Background: 99968\n",
      "[Test]  Signals: 24992, Background: 24992\n"
     ]
    }
   ],
   "source": [
    "# Define binary labels for training set\n",
    "train_signal_label = np.min(y_train_full)\n",
    "y_train = (y_train_full == train_signal_label).astype(int)\n",
    "\n",
    "# Save full test set before filtering\n",
    "X_test_full = X_test.copy()\n",
    "y_test_full_labels = y_test_full.copy()\n",
    "context_test_full = context_test.copy()\n",
    "\n",
    "# Define binary labels and filter for test set\n",
    "test_signal_label = 8\n",
    "test_background_labels = [10, 11]\n",
    "test_mask = np.isin(y_test_full, [test_signal_label] + test_background_labels)\n",
    "\n",
    "X_test = X_test[test_mask]\n",
    "y_test_full = y_test_full[test_mask]\n",
    "y_test = (y_test_full == test_signal_label).astype(int)\n",
    "context_test = context_test[test_mask]  # <-- Apply same mask to context\n",
    "\n",
    "print(f\"[Train] Signals: {np.sum(y_train==1)}, Background: {np.sum(y_train==0)}\")\n",
    "print(f\"[Test]  Signals: {np.sum(y_test==1)}, Background: {np.sum(y_test==0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "2bcec9a9-a1e9-4bb0-a772-395c5826c958",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "# Concatenate embedding and context as input features\n",
    "X_train_combined = np.concatenate([X_train, context_train], axis=1)\n",
    "X_test_combined = np.concatenate([X_test, context_test], axis=1)\n",
    "X_bkg_combined = np.concatenate([X_bkg, context_bkg], axis=1)  # NEW\n",
    "\n",
    "# Split training set into train and validation\n",
    "X_tr, X_val, y_tr, y_val = train_test_split(\n",
    "    X_train_combined, y_train, test_size=0.2, random_state=42, stratify=y_train\n",
    ")\n",
    "\n",
    "# Create TensorDatasets\n",
    "train_ds = TensorDataset(torch.tensor(X_tr, dtype=torch.float32),\n",
    "                         torch.tensor(y_tr, dtype=torch.float32))\n",
    "val_ds = TensorDataset(torch.tensor(X_val, dtype=torch.float32),\n",
    "                       torch.tensor(y_val, dtype=torch.float32))\n",
    "test_ds = TensorDataset(torch.tensor(X_test_combined, dtype=torch.float32),\n",
    "                        torch.tensor(y_test, dtype=torch.float32))\n",
    "bkg_ds = TensorDataset(torch.tensor(X_bkg_combined, dtype=torch.float32),\n",
    "                       torch.tensor(y_bkg_full, dtype=torch.float32))  # NEW\n",
    "\n",
    "# DataLoaders\n",
    "train_loader = DataLoader(train_ds, batch_size=128, shuffle=True)\n",
    "val_loader = DataLoader(val_ds, batch_size=128)\n",
    "test_loader = DataLoader(test_ds, batch_size=128)\n",
    "bkg_loader = DataLoader(bkg_ds, batch_size=128)  # NEW\n",
    "\n",
    "# Also save test context tensor separately for NF usage\n",
    "context_train_tensor = torch.tensor(context_train, dtype=torch.float32)\n",
    "context_test_tensor = torch.tensor(context_test, dtype=torch.float32)\n",
    "context_bkg_tensor = torch.tensor(context_bkg, dtype=torch.float32)  # NEW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "93a08348-3e02-4698-aca6-8af9134dce28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training set feature summary:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>min</th>\n",
       "      <th>max</th>\n",
       "      <th>mean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>feat_0</th>\n",
       "      <td>-0.906202</td>\n",
       "      <td>1.768859</td>\n",
       "      <td>0.296277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feat_1</th>\n",
       "      <td>-1.828741</td>\n",
       "      <td>0.331600</td>\n",
       "      <td>-0.955492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feat_2</th>\n",
       "      <td>-0.270942</td>\n",
       "      <td>2.227935</td>\n",
       "      <td>1.232510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feat_3</th>\n",
       "      <td>-3.631402</td>\n",
       "      <td>-0.525541</td>\n",
       "      <td>-2.478959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feat_4</th>\n",
       "      <td>-0.792154</td>\n",
       "      <td>1.708964</td>\n",
       "      <td>0.566783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feat_5</th>\n",
       "      <td>-2.176033</td>\n",
       "      <td>0.663829</td>\n",
       "      <td>-1.135372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feat_6</th>\n",
       "      <td>-2.957600</td>\n",
       "      <td>0.499910</td>\n",
       "      <td>-0.885186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feat_7</th>\n",
       "      <td>-6.715720</td>\n",
       "      <td>-2.348159</td>\n",
       "      <td>-5.540005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feat_8</th>\n",
       "      <td>-0.803847</td>\n",
       "      <td>0.340612</td>\n",
       "      <td>-0.237461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feat_9</th>\n",
       "      <td>-2.548387</td>\n",
       "      <td>0.716190</td>\n",
       "      <td>-0.864265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feat_10</th>\n",
       "      <td>-0.857938</td>\n",
       "      <td>0.973803</td>\n",
       "      <td>0.357465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feat_11</th>\n",
       "      <td>-1.039342</td>\n",
       "      <td>1.914268</td>\n",
       "      <td>0.602340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feat_12</th>\n",
       "      <td>-1.206306</td>\n",
       "      <td>1.175586</td>\n",
       "      <td>-0.277959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feat_13</th>\n",
       "      <td>-0.759644</td>\n",
       "      <td>2.185685</td>\n",
       "      <td>1.261059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feat_14</th>\n",
       "      <td>-2.113556</td>\n",
       "      <td>0.527495</td>\n",
       "      <td>-1.048521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feat_15</th>\n",
       "      <td>-0.493412</td>\n",
       "      <td>2.221478</td>\n",
       "      <td>1.133696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feat_16</th>\n",
       "      <td>-0.373480</td>\n",
       "      <td>0.358626</td>\n",
       "      <td>0.000056</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              min       max      mean\n",
       "feat_0  -0.906202  1.768859  0.296277\n",
       "feat_1  -1.828741  0.331600 -0.955492\n",
       "feat_2  -0.270942  2.227935  1.232510\n",
       "feat_3  -3.631402 -0.525541 -2.478959\n",
       "feat_4  -0.792154  1.708964  0.566783\n",
       "feat_5  -2.176033  0.663829 -1.135372\n",
       "feat_6  -2.957600  0.499910 -0.885186\n",
       "feat_7  -6.715720 -2.348159 -5.540005\n",
       "feat_8  -0.803847  0.340612 -0.237461\n",
       "feat_9  -2.548387  0.716190 -0.864265\n",
       "feat_10 -0.857938  0.973803  0.357465\n",
       "feat_11 -1.039342  1.914268  0.602340\n",
       "feat_12 -1.206306  1.175586 -0.277959\n",
       "feat_13 -0.759644  2.185685  1.261059\n",
       "feat_14 -2.113556  0.527495 -1.048521\n",
       "feat_15 -0.493412  2.221478  1.133696\n",
       "feat_16 -0.373480  0.358626  0.000056"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set (signals) feature summary:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>min</th>\n",
       "      <th>max</th>\n",
       "      <th>mean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>feat_0</th>\n",
       "      <td>-0.884004</td>\n",
       "      <td>1.712306</td>\n",
       "      <td>0.293583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feat_1</th>\n",
       "      <td>-1.702500</td>\n",
       "      <td>0.385448</td>\n",
       "      <td>-0.953610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feat_2</th>\n",
       "      <td>-0.277713</td>\n",
       "      <td>2.224108</td>\n",
       "      <td>1.231197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feat_3</th>\n",
       "      <td>-3.562857</td>\n",
       "      <td>-0.865574</td>\n",
       "      <td>-2.478424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feat_4</th>\n",
       "      <td>-0.727057</td>\n",
       "      <td>1.710533</td>\n",
       "      <td>0.566758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feat_5</th>\n",
       "      <td>-2.194729</td>\n",
       "      <td>0.666447</td>\n",
       "      <td>-1.134076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feat_6</th>\n",
       "      <td>-2.957579</td>\n",
       "      <td>0.447521</td>\n",
       "      <td>-0.887502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feat_7</th>\n",
       "      <td>-6.692633</td>\n",
       "      <td>-2.577528</td>\n",
       "      <td>-5.540372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feat_8</th>\n",
       "      <td>-0.830857</td>\n",
       "      <td>0.341462</td>\n",
       "      <td>-0.237391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feat_9</th>\n",
       "      <td>-2.464218</td>\n",
       "      <td>0.676337</td>\n",
       "      <td>-0.861810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feat_10</th>\n",
       "      <td>-0.858419</td>\n",
       "      <td>0.921535</td>\n",
       "      <td>0.356324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feat_11</th>\n",
       "      <td>-0.948162</td>\n",
       "      <td>1.970664</td>\n",
       "      <td>0.604820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feat_12</th>\n",
       "      <td>-1.261090</td>\n",
       "      <td>1.289225</td>\n",
       "      <td>-0.278266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feat_13</th>\n",
       "      <td>-0.759644</td>\n",
       "      <td>2.164363</td>\n",
       "      <td>1.259431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feat_14</th>\n",
       "      <td>-2.049291</td>\n",
       "      <td>0.527495</td>\n",
       "      <td>-1.046132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feat_15</th>\n",
       "      <td>-0.354010</td>\n",
       "      <td>2.222293</td>\n",
       "      <td>1.132630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feat_16</th>\n",
       "      <td>-0.286200</td>\n",
       "      <td>0.382235</td>\n",
       "      <td>0.000031</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              min       max      mean\n",
       "feat_0  -0.884004  1.712306  0.293583\n",
       "feat_1  -1.702500  0.385448 -0.953610\n",
       "feat_2  -0.277713  2.224108  1.231197\n",
       "feat_3  -3.562857 -0.865574 -2.478424\n",
       "feat_4  -0.727057  1.710533  0.566758\n",
       "feat_5  -2.194729  0.666447 -1.134076\n",
       "feat_6  -2.957579  0.447521 -0.887502\n",
       "feat_7  -6.692633 -2.577528 -5.540372\n",
       "feat_8  -0.830857  0.341462 -0.237391\n",
       "feat_9  -2.464218  0.676337 -0.861810\n",
       "feat_10 -0.858419  0.921535  0.356324\n",
       "feat_11 -0.948162  1.970664  0.604820\n",
       "feat_12 -1.261090  1.289225 -0.278266\n",
       "feat_13 -0.759644  2.164363  1.259431\n",
       "feat_14 -2.049291  0.527495 -1.046132\n",
       "feat_15 -0.354010  2.222293  1.132630\n",
       "feat_16 -0.286200  0.382235  0.000031"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set (background) feature summary:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>min</th>\n",
       "      <th>max</th>\n",
       "      <th>mean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>feat_0</th>\n",
       "      <td>-0.923425</td>\n",
       "      <td>1.544518</td>\n",
       "      <td>0.075557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feat_1</th>\n",
       "      <td>-1.602362</td>\n",
       "      <td>0.356237</td>\n",
       "      <td>-0.830534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feat_2</th>\n",
       "      <td>-0.286450</td>\n",
       "      <td>2.032506</td>\n",
       "      <td>1.101465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feat_3</th>\n",
       "      <td>-3.369917</td>\n",
       "      <td>-0.613869</td>\n",
       "      <td>-2.414099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feat_4</th>\n",
       "      <td>-0.765827</td>\n",
       "      <td>1.727490</td>\n",
       "      <td>0.595974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feat_5</th>\n",
       "      <td>-1.965027</td>\n",
       "      <td>0.666447</td>\n",
       "      <td>-1.001327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feat_6</th>\n",
       "      <td>-2.957600</td>\n",
       "      <td>0.246266</td>\n",
       "      <td>-1.106921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feat_7</th>\n",
       "      <td>-6.462726</td>\n",
       "      <td>-2.335717</td>\n",
       "      <td>-5.530254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feat_8</th>\n",
       "      <td>-0.782748</td>\n",
       "      <td>0.341462</td>\n",
       "      <td>-0.226501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feat_9</th>\n",
       "      <td>-2.187151</td>\n",
       "      <td>0.736296</td>\n",
       "      <td>-0.677595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feat_10</th>\n",
       "      <td>-0.771854</td>\n",
       "      <td>0.891261</td>\n",
       "      <td>0.283138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feat_11</th>\n",
       "      <td>-0.658631</td>\n",
       "      <td>1.910001</td>\n",
       "      <td>0.765876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feat_12</th>\n",
       "      <td>-1.205759</td>\n",
       "      <td>1.251136</td>\n",
       "      <td>-0.264815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feat_13</th>\n",
       "      <td>-0.759644</td>\n",
       "      <td>1.999831</td>\n",
       "      <td>1.116605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feat_14</th>\n",
       "      <td>-2.032815</td>\n",
       "      <td>0.542403</td>\n",
       "      <td>-0.866648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feat_15</th>\n",
       "      <td>-0.460019</td>\n",
       "      <td>2.051895</td>\n",
       "      <td>1.017708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feat_16</th>\n",
       "      <td>-0.316513</td>\n",
       "      <td>0.345902</td>\n",
       "      <td>0.000131</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              min       max      mean\n",
       "feat_0  -0.923425  1.544518  0.075557\n",
       "feat_1  -1.602362  0.356237 -0.830534\n",
       "feat_2  -0.286450  2.032506  1.101465\n",
       "feat_3  -3.369917 -0.613869 -2.414099\n",
       "feat_4  -0.765827  1.727490  0.595974\n",
       "feat_5  -1.965027  0.666447 -1.001327\n",
       "feat_6  -2.957600  0.246266 -1.106921\n",
       "feat_7  -6.462726 -2.335717 -5.530254\n",
       "feat_8  -0.782748  0.341462 -0.226501\n",
       "feat_9  -2.187151  0.736296 -0.677595\n",
       "feat_10 -0.771854  0.891261  0.283138\n",
       "feat_11 -0.658631  1.910001  0.765876\n",
       "feat_12 -1.205759  1.251136 -0.264815\n",
       "feat_13 -0.759644  1.999831  1.116605\n",
       "feat_14 -2.032815  0.542403 -0.866648\n",
       "feat_15 -0.460019  2.051895  1.017708\n",
       "feat_16 -0.316513  0.345902  0.000131"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def summarize_features(X, name):\n",
    "    df = pd.DataFrame(X, columns=[f'feat_{i}' for i in range(X.shape[1])])\n",
    "    summary = pd.DataFrame({\n",
    "        'min': df.min(),\n",
    "        'max': df.max(),\n",
    "        'mean': df.mean()\n",
    "    })\n",
    "    print(f\"\\n{name} feature summary:\")\n",
    "    display(summary)\n",
    "\n",
    "# Summarize training features\n",
    "summarize_features(X_train_combined, \"Training set\")\n",
    "\n",
    "# Summarize test signal features\n",
    "summarize_features(X_test_combined, \"Test set (signals)\")\n",
    "\n",
    "# Summarize test background features\n",
    "summarize_features(X_bkg_combined, \"Test set (background)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "cea1c950-22a5-4895-ab76-1145c7b880f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define better MLP classifier\n",
    "# class BetterMLPClassifier(nn.Module):\n",
    "#     def __init__(self, input_dim):\n",
    "#         super().__init__()\n",
    "#         self.model = nn.Sequential(\n",
    "#             nn.Linear(input_dim, 256),\n",
    "#             nn.BatchNorm1d(256),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Dropout(0.3),\n",
    "#             nn.Linear(256, 128),\n",
    "#             nn.BatchNorm1d(128),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Dropout(0.3),\n",
    "#             nn.Linear(128, 64),\n",
    "#             nn.BatchNorm1d(64),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Linear(64, 1)\n",
    "#         )\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         return self.model(x).squeeze()\n",
    "\n",
    "\n",
    "# # Margin-based binary contrastive loss\n",
    "# class MarginBinaryLoss(nn.Module):\n",
    "#     def __init__(self, margin=0.2):\n",
    "#         super().__init__()\n",
    "#         self.margin = margin\n",
    "#         self.bce = nn.BCEWithLogitsLoss()\n",
    "\n",
    "#     def forward(self, logits, targets):\n",
    "#         loss = self.bce(logits, targets)\n",
    "\n",
    "#         signal_mask = targets == 1\n",
    "#         background_mask = targets == 0\n",
    "\n",
    "#         if signal_mask.sum() > 0 and background_mask.sum() > 0:\n",
    "#             signal_logits = logits[signal_mask]\n",
    "#             background_logits = logits[background_mask]\n",
    "\n",
    "#             hardest_background = background_logits.max()\n",
    "#             margin_loss = torch.relu(self.margin - (signal_logits - hardest_background)).mean()\n",
    "\n",
    "#             loss += margin_loss\n",
    "#         return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "7f8760f3-b165-4c0e-b0b6-25642519c85c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # Setup device, model, optimizer, loss, scaler, and scheduler\n",
    "# device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "# model = BetterMLPClassifier(input_dim=X_train_combined.shape[1]).to(device)\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "# criterion = MarginBinaryLoss(margin=0.2)\n",
    "# scaler = torch.cuda.amp.GradScaler(enabled=(device == 'cuda'))\n",
    "# scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=3, verbose=True)\n",
    "\n",
    "# # Training loop with validation loss\n",
    "# for epoch in range(1, 21):\n",
    "#     model.train()\n",
    "#     running_loss = 0.0\n",
    "\n",
    "#     for xb, yb in train_loader:\n",
    "#         xb, yb = xb.to(device), yb.to(device)\n",
    "\n",
    "#         optimizer.zero_grad()\n",
    "#         with torch.cuda.amp.autocast(enabled=(device == 'cuda')):\n",
    "#             logits = model(xb)\n",
    "#             loss = criterion(logits, yb)\n",
    "\n",
    "#         scaler.scale(loss).backward()\n",
    "#         scaler.step(optimizer)\n",
    "#         scaler.update()\n",
    "\n",
    "#         running_loss += loss.item()\n",
    "\n",
    "#     avg_loss = running_loss / len(train_loader)\n",
    "\n",
    "#     # Validation loss\n",
    "#     model.eval()\n",
    "#     val_loss = 0.0\n",
    "#     with torch.no_grad():\n",
    "#         for xb, yb in val_loader:\n",
    "#             xb, yb = xb.to(device), yb.to(device)\n",
    "#             with torch.cuda.amp.autocast(enabled=(device == 'cuda')):\n",
    "#                 logits = model(xb)\n",
    "#                 loss = criterion(logits, yb)\n",
    "#             val_loss += loss.item()\n",
    "\n",
    "#     avg_val_loss = val_loss / len(val_loader)\n",
    "#     scheduler.step(avg_val_loss)\n",
    "\n",
    "#     print(f\"Epoch {epoch:02d}, Train Loss: {avg_loss:.4f}, Val Loss: {avg_val_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "5bd964d3-4109-4761-84d2-8f2d10058f45",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/local/katya.govorkova/ipykernel_95553/2762259159.py:89: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler(enabled=(device == 'cuda'))\n",
      "/local/katya.govorkova/ipykernel_95553/2762259159.py:104: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=(device == 'cuda')):\n",
      "/local/katya.govorkova/ipykernel_95553/2762259159.py:131: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=(device == 'cuda')):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01, Train Loss: 0.2504, Val Loss: 0.1927\n",
      "Epoch 02, Train Loss: 0.2389, Val Loss: 0.2906\n",
      "Epoch 03, Train Loss: 0.2625, Val Loss: 0.1991\n",
      "Epoch 04, Train Loss: 0.2800, Val Loss: 0.2123\n",
      "Epoch 05, Train Loss: 0.2853, Val Loss: 0.2381\n",
      "Epoch 06, Train Loss: 0.2882, Val Loss: 0.2535\n",
      "Epoch 07, Train Loss: 0.2864, Val Loss: 0.2820\n",
      "Epoch 08, Train Loss: 0.2854, Val Loss: 0.2726\n",
      "Epoch 09, Train Loss: 0.2873, Val Loss: 0.2420\n",
      "Epoch 10, Train Loss: 0.2899, Val Loss: 0.2400\n",
      "Epoch 11, Train Loss: 0.2772, Val Loss: 0.2816\n",
      "Epoch 12, Train Loss: 0.2750, Val Loss: 0.2550\n",
      "Epoch 13, Train Loss: 0.2730, Val Loss: 0.2695\n",
      "Epoch 14, Train Loss: 0.2840, Val Loss: 0.2842\n",
      "Epoch 15, Train Loss: 0.2711, Val Loss: 0.2647\n",
      "Epoch 16, Train Loss: 0.2706, Val Loss: 0.2574\n",
      "Epoch 17, Train Loss: 0.2652, Val Loss: 0.2728\n",
      "Epoch 18, Train Loss: 0.2882, Val Loss: 0.2569\n",
      "Epoch 19, Train Loss: 0.2722, Val Loss: 0.2464\n",
      "Epoch 20, Train Loss: 0.2744, Val Loss: 0.2374\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Define better MLP classifier\n",
    "class BetterMLPClassifier(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_dim, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x).squeeze()\n",
    "\n",
    "# Focal loss definition\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, alpha=0.25, gamma=2.0, reduction='mean'):\n",
    "        super().__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.reduction = reduction\n",
    "\n",
    "    def forward(self, logits, targets):\n",
    "        targets = targets.float()\n",
    "        bce_loss = F.binary_cross_entropy_with_logits(logits, targets, reduction='none')\n",
    "        probas = torch.sigmoid(logits)\n",
    "        p_t = probas * targets + (1 - probas) * (1 - targets)\n",
    "        alpha_t = self.alpha * targets + (1 - self.alpha) * (1 - targets)\n",
    "        focal_loss = alpha_t * (1 - p_t) ** self.gamma * bce_loss\n",
    "\n",
    "        if self.reduction == 'mean':\n",
    "            return focal_loss.mean()\n",
    "        elif self.reduction == 'sum':\n",
    "            return focal_loss.sum()\n",
    "        else:\n",
    "            return focal_loss\n",
    "\n",
    "# Combined classifier + NF loss\n",
    "class ClassifierNFJointLoss(nn.Module):\n",
    "    def __init__(self, margin=0.2, alpha=1.0, beta=1.0, focal_alpha=0.25, focal_gamma=2.0):\n",
    "        super().__init__()\n",
    "        self.margin = margin\n",
    "        self.alpha = alpha  # weight for margin loss\n",
    "        self.beta = beta    # weight for NF penalty\n",
    "        self.focal = FocalLoss(alpha=focal_alpha, gamma=focal_gamma)\n",
    "\n",
    "    def forward(self, logits, targets, nf_log_likelihood):\n",
    "        targets = targets.float()\n",
    "        loss = self.focal(logits, targets)\n",
    "\n",
    "        signal_mask = targets == 1\n",
    "        background_mask = targets == 0\n",
    "\n",
    "        if signal_mask.sum() > 0 and background_mask.sum() > 0:\n",
    "            signal_logits = logits[signal_mask]\n",
    "            background_logits = logits[background_mask]\n",
    "            hardest_background = background_logits.max()\n",
    "            margin_loss = torch.relu(self.margin - (signal_logits - hardest_background)).mean()\n",
    "            loss += self.alpha * margin_loss\n",
    "\n",
    "        if nf_log_likelihood is not None:\n",
    "            nf_bkg = nf_log_likelihood[background_mask]\n",
    "            nf_signal = nf_log_likelihood[signal_mask]\n",
    "            nf_loss = (nf_signal.mean() - nf_bkg.mean())\n",
    "            loss -= self.beta * nf_loss\n",
    "\n",
    "        return loss\n",
    "\n",
    "# Setup device, model, optimizer, loss, scaler, and scheduler\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model = BetterMLPClassifier(input_dim=X_train_combined.shape[1]).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "# Use joint classifier + NF loss with focal loss\n",
    "criterion = ClassifierNFJointLoss(margin=0.2, alpha=1.0, beta=0.01)\n",
    "\n",
    "scaler = torch.cuda.amp.GradScaler(enabled=(device == 'cuda'))\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=3, verbose=True)\n",
    "\n",
    "# Training loop with validation loss\n",
    "for epoch in range(1, 21):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for xb, yb in train_loader:\n",
    "        xb, yb = xb.to(device), yb.to(device)\n",
    "\n",
    "        x_emb = xb[:, :-1]  # embedding\n",
    "        x_ctx = xb[:, -1:]  # context\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        with torch.cuda.amp.autocast(enabled=(device == 'cuda')):\n",
    "            logits = model(xb)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                nf_ll = -1 * nf_model(x_emb, x_ctx)\n",
    "\n",
    "            loss = criterion(logits, yb, nf_ll)\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=5.0)\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    avg_loss = running_loss / len(train_loader)\n",
    "\n",
    "    # Validation loss\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in val_loader:\n",
    "            xb, yb = xb.to(device), yb.to(device)\n",
    "\n",
    "            x_emb = xb[:, :-1]\n",
    "            x_ctx = xb[:, -1:]\n",
    "\n",
    "            with torch.cuda.amp.autocast(enabled=(device == 'cuda')):\n",
    "                logits = model(xb)\n",
    "                nf_ll = -1 * nf_model(x_emb, x_ctx)\n",
    "                loss = criterion(logits, yb, nf_ll)\n",
    "\n",
    "            val_loss += loss.item()\n",
    "\n",
    "    avg_val_loss = val_loss / len(val_loader)\n",
    "    scheduler.step(avg_val_loss)\n",
    "    print(f\"Epoch {epoch:02d}, Train Loss: {avg_loss:.4f}, Val Loss: {avg_val_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a181d3c5-6fd1-4656-82a6-99392fa99014",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate classifier on test set\n",
    "model.eval()\n",
    "y_test_logits, y_test_probs, y_test_true = [], [], []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for xb, yb in test_loader:\n",
    "        xb = xb.to(device)\n",
    "        logits = model(xb)\n",
    "        probs = torch.sigmoid(logits)\n",
    "\n",
    "        y_test_logits.append(logits.cpu().numpy())\n",
    "        y_test_probs.append(probs.cpu().numpy())\n",
    "        y_test_true.append(yb.numpy())\n",
    "\n",
    "# Concatenate results\n",
    "y_test_logits = np.concatenate(y_test_logits)\n",
    "y_test_probs = np.concatenate(y_test_probs)\n",
    "y_test_true = np.concatenate(y_test_true)\n",
    "\n",
    "# Metrics\n",
    "roc_auc = roc_auc_score(y_test_true, y_test_probs)\n",
    "acc = accuracy_score(y_test_true, y_test_probs > 0.5)\n",
    "\n",
    "print(f\"[TEST] AUC: {roc_auc:.3f}, Accuracy: {acc:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e1e4260-dadd-4907-af81-ba2455004ecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "bkg_probs = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for xb, _ in bkg_loader:\n",
    "        xb = xb.to(device)\n",
    "        probs = torch.sigmoid(model(xb))\n",
    "        bkg_probs.append(probs.cpu().numpy())\n",
    "\n",
    "bkg_probs = np.concatenate(bkg_probs)\n",
    "\n",
    "plt.hist(bkg_probs, bins=50, color='orange', alpha=0.7)\n",
    "plt.title(\"Classifier Scores on Background-Only Set\")\n",
    "plt.xlabel(\"Score\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Background scores > 0.5: {(bkg_probs > 0.5).sum()} / {len(bkg_probs)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c392316-0dd5-405c-8e28-468899add3ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load trained NF\n",
    "nf_model = torch.jit.load(\"/home/katya.govorkova/gwak2/gwak/output/ResNet_NF_from_file_conditioning_HL/model_JIT.pt\").eval().to(device)\n",
    "\n",
    "# Evaluate NF log-probabilities on test set\n",
    "nf_scores = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i in range(0, len(X_test), 512):\n",
    "        xb = torch.tensor(X_test[i:i+512], dtype=torch.float32).to(device)           # standardized embeddings\n",
    "        ctx = context_test_tensor[i:i+512].to(device)                                # corresponding context\n",
    "        log_probs = nf_model(xb, context=ctx) * (-1)                               # assumes .log_prob(x, context=...)\n",
    "        nf_scores.append(log_probs.cpu().numpy())\n",
    "\n",
    "nf_scores = np.concatenate(nf_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eef2409-75b4-4807-8fcb-dd131ab6b090",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "\n",
    "# Signal of interest\n",
    "signal_label = 8\n",
    "background_labels = [10, 11]\n",
    "\n",
    "# Mask for selected signal + background\n",
    "mask = np.isin(y_test_full, [signal_label] + background_labels)\n",
    "y_bin_true = (y_test_full[mask] == signal_label).astype(int)\n",
    "\n",
    "# Classifier-based scores\n",
    "clf_scores = y_test_probs[mask]\n",
    "\n",
    "# NF anomaly scores (lower = more anomalous, so invert)\n",
    "nf_anomaly_scores = nf_scores[mask]\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(8, 6))\n",
    "\n",
    "# Classifier ROC\n",
    "fpr_clf, tpr_clf, _ = roc_curve(y_bin_true, clf_scores)\n",
    "auc_clf = roc_auc_score(y_bin_true, clf_scores)\n",
    "plt.plot(fpr_clf, tpr_clf, label=f\"Classifier (AUC={auc_clf:.2f})\")\n",
    "\n",
    "# NF ROC\n",
    "fpr_nf, tpr_nf, _ = roc_curve(y_bin_true, nf_anomaly_scores)\n",
    "auc_nf = roc_auc_score(y_bin_true, nf_anomaly_scores)\n",
    "plt.plot(fpr_nf, tpr_nf, label=f\"NF (AUC={auc_nf:.2f})\")\n",
    "\n",
    "# Plot formatting\n",
    "plt.plot([0, 1], [0, 1], '--', color='gray')\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(\"ROC Curve for Signal 8 (WNB) vs Background (10,11)\")\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c804c528-a7e9-40a2-b08d-85e54761aea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log-scale histogram of predicted probabilities\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.hist(y_test_probs[y_test_true == 1], bins=100, alpha=0.6, label='Signal', density=True)\n",
    "plt.hist(y_test_probs[y_test_true == 0], bins=100, alpha=0.6, label='Background', density=True)\n",
    "plt.xscale('log')\n",
    "plt.xlabel('Prediction Score (log scale)')\n",
    "plt.ylabel('Density')\n",
    "plt.title('Classifier Score Tail (log scale)')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14759345-9729-4cbb-ad56-86869cf1b18c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create subplots\n",
    "fig, axs = plt.subplots(1, 2, figsize=(14, 5), sharey=False)\n",
    "\n",
    "# --- Left: Classifier ---\n",
    "axs[0].hist(y_test_probs[y_test_true == 1], bins=100, alpha=0.6, label=f'Signal, {max(y_test_probs[y_test_true == 1]):.2f}', density=True)\n",
    "axs[0].hist(y_test_probs[y_test_true == 0], bins=100, alpha=0.6, label=f'Background, {max(y_test_probs[y_test_true == 0]):.2f}', density=True)\n",
    "axs[0].set_xlabel('Classifier Score (log scale)')\n",
    "axs[0].set_ylabel('Density')\n",
    "axs[0].set_title('Classifier Output')\n",
    "axs[0].legend()\n",
    "axs[0].grid(True)\n",
    "\n",
    "# --- Right: NF (clipped) ---\n",
    "axs[1].hist(nf_scores[y_test_true == 1], bins=100, alpha=0.6, label=f'Signal, {max(nf_scores[y_test_true == 1]):.2f}', density=True)\n",
    "axs[1].hist(nf_scores[y_test_true == 0], bins=100, alpha=0.6, label=f'Background, {max(nf_scores[y_test_true == 0]):.2f}', density=True)\n",
    "axs[1].set_xlabel('âˆ’Log-Likelihood (log scale)')\n",
    "axs[1].set_title('Normalizing Flow Output')\n",
    "axs[1].legend()\n",
    "axs[1].grid(True)\n",
    "\n",
    "plt.suptitle(\"Signal vs Background Score Distributions (log-scale)\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bb18b4e-8e5b-407a-b056-154f425608b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "min(y_test_probs[y_test_true == 1]), max(y_test_probs[y_test_true == 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c68b016-c2d5-4db0-a71e-83f033b0c503",
   "metadata": {},
   "outputs": [],
   "source": [
    "min(y_test_probs[y_test_true == 0]), max(y_test_probs[y_test_true == 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89b715aa-8919-4bca-88f8-4399ae2306de",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gwak.train.plotting import make_corner\n",
    "fig = make_corner(X_train, (y_train).astype(int), return_fig=True, label_names=['WNB', 'Background'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93197404-f632-4d42-939c-54f37ea399b0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
